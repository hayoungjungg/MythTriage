{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038b0907",
   "metadata": {},
   "source": [
    "## LLM Prompt Engineering Pipeline\n",
    "- Sampling data for prompt-engineering and few-shot examples\n",
    "- Programming LLM prediction pipeline (per myth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f7dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "from utils import prompts, EvaluatorHelper, GPTRequests\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from dotenv import dotenv_values\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "random.seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcccf689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing OpenAI Client\n",
    "secrets = dotenv_values(\".env\")\n",
    "api_key = secrets['OPENAI_KEY']\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21f0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "FULL_EVAL_DIR =  '/home/hjung10/oud-audit/labeling-pipeline/myth-eval-data/evaluation_set/'\n",
    "MYTH_TO_EVAL_FILE = {'M1': 'M1_evaluation_set.csv',\n",
    "                     'M2': 'M2_evaluation_set.csv',\n",
    "                     'M3': 'M3_evaluation_set.csv',\n",
    "                     'M4': 'M4_evaluation_set.csv',\n",
    "                     'M5': 'M5_evaluation_set.csv',\n",
    "                     'M6': 'M6_evaluation_set.csv',\n",
    "                     'M7': 'M7_evaluation_set.csv',\n",
    "                     'M8': 'M8_evaluation_set.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a1c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the video_id to label mapping from the evaluation set\n",
    "\"\"\"\n",
    "def create_vid_to_label_eval(dataframe):\n",
    "    vid_to_label = defaultdict()\n",
    "\n",
    "    # iterating through each row\n",
    "    for i, row in dataframe.iterrows():\n",
    "        vid_to_label[row['video_id']] = row['label']\n",
    "        \n",
    "    return vid_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d913b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Videos to exclude as it is included in the few-shot example\n",
    "MYTH_TO_FEW_SHOT_EXCLUDE = {\n",
    "    'M1' : ['SjCZwqEE22Y', '7PT0gv6a97o', 'X3UKcHR-2uU', 'fTcGyWDDg5s', 'bMitni3tC-c'],\n",
    "    'M2' : ['9TYr6sqDEfY', 'DyjRxf-aJN4', 'AnUN2Zs4Mnk', 'm_uV8UkTDKw', '-3G162dqVVI'],\n",
    "    'M3' : ['Jc-buPCKisM', '0hR2Hwkhey8', 'UfQWOGOFNFA', 'JczoO7ogOS8', 'SjCZwqEE22Y'],\n",
    "    'M4' : ['DyjRxf-aJN4', '7PT0gv6a97o', 'Qg7G0VTi3iY', 'OFGFeA6Ap7E', 'v4GnSSvcYys'],\n",
    "    'M5' : ['zN9NDZ6lgaM', '7PT0gv6a97o', 'sZ5-i72Yl2Q', 'FmGalSsq63k', 'TnYHKxUHgCs'],\n",
    "    'M6' : ['tzHKfZyevXo', 'eWdCJm9q1bw', '7gtWuoWGQWM', 'WNXieqey_iQ', 'SjCZwqEE22Y'],\n",
    "    'M7' : ['QtRQ9UD7rpY', 'W-7_alg4I28', '0RkpSTlvvj0', '0y55ymuJ2K4', 'TP0ToVYXQ-k'],\n",
    "    'M10' : ['6F6d10ggVDw', 'nmMCQ1y8l14', 'GI3blNNe56w', 'E9jKyHjPbUg', 'QtRQ9UD7rpY']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d3f16",
   "metadata": {},
   "source": [
    "### Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9557039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myth statements\n",
    "MYTH_TO_STATEMENT = {\n",
    "    'M1': 'Agonist therapy or medication-assisted treatment (MAT) for OUD is merely replacing one drug with another.',\n",
    "    'M2': 'People with OUD are not suffering from a medical DISEASE treatable with medication from a self-imposed condition maintained through the lack of moral fiber.', \n",
    "    'M3': 'The ultimate goal of treatment for OUD is abstinence from any opioid use (e.g., Taking medication is not true recovery).',\n",
    "    'M4': 'Only patients with certain characteristics are vulnerable to addiction.',\n",
    "    'M5': 'Physical dependence or tolerance is the same as addiction.',\n",
    "    'M6': 'Detoxification for OUD is effective.',\n",
    "    'M7': 'You should only take medication for a brief period of time.',\n",
    "    'M8': 'Kratom is a non-addictive and safe alternative to opioids.'\n",
    "}\n",
    "\n",
    "# variables\n",
    "myth_key = 'M1'\n",
    "model_name = 'gpt-4o-2024-08-06' # e.g., gpt-4o-2024-08-06, gpt-4o-mini-2024-07-18\n",
    "temperature = 0.2   #  fixed based on prior works (which shows this as the optimal temperature for classification)\n",
    "vid_to_output = defaultdict()\n",
    "\n",
    "# zero-shot only (CHANGE FOR FEW SHOT)\n",
    "prompt_chosen = prompts.zero_shot_prompt\n",
    "\n",
    "# directories\n",
    "EVAL_DIR = os.getcwd() + '/evaluations/evaluation-set/'\n",
    "df = pd.read_csv(FULL_EVAL_DIR + MYTH_TO_EVAL_FILE[myth_key])\n",
    "save_file_dir_EVAL = EVAL_DIR + model_name + '-' + myth_key + '-zero-shot-evaluation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b8fbbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading the data & creating the evaluation dictionary\n",
    "vid_to_label_eval = create_vid_to_label_eval(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crafting the prompt\n",
    "myth = MYTH_TO_STATEMENT[myth_key]\n",
    "crafted_prompt = EvaluatorHelper.create_myth_specific_prompts(df, prompt_chosen, prompts.persona, myth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding the prompt into the LLM and collecting the output\n",
    "vid_to_output, total_completion_token, total_prompt_token = GPTRequests.evaluate_prompts(crafted_prompt, model_name, temperature, vid_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "310fdc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the list to a JSON file\n",
    "GPTRequests.extract_and_save_output(save_file_dir_EVAL, vid_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_dir_EVAL = EVAL_DIR + model_name + '-' + myth_key + '-zero-shot-evaluation.json'\n",
    "print(save_file_dir_EVAL)\n",
    "with open(save_file_dir_EVAL, 'r') as json_file:\n",
    "    vid_to_output = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluatorHelper.compute_results(vid_to_output, vid_to_label_eval, False, MYTH_TO_FEW_SHOT_EXCLUDE[myth_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414c1d1",
   "metadata": {},
   "source": [
    "### Few-Shot Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e658b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "myth_key = 'M2'\n",
    "model_name = 'gpt-4o-2024-08-06' # e.g., gpt-4o-2024-08-06\n",
    "temperature = 0.2   #  fixed based on prior works (which shows this as the optimal temperature for classification)\n",
    "vid_to_output = defaultdict()\n",
    "\n",
    "# zero-shot only (CHANGE FOR FEW SHOT)\n",
    "prompt_chosen = prompts.few_shot_prompt\n",
    "\n",
    "# directories\n",
    "EVAL_DIR = os.getcwd() + '/evaluations/evaluation-set/'\n",
    "\n",
    "# prompt engineering vs. eval\n",
    "df = pd.read_csv(FULL_EVAL_DIR + MYTH_TO_EVAL_FILE[myth_key])\n",
    "save_file_dir_EVAL = EVAL_DIR + model_name + '-' + myth_key + '-few-shot-evaluation.json'\n",
    "\n",
    "# reading the data & creating the evaluation dictionary\n",
    "vid_to_label_eval = create_vid_to_label_eval(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49dc4409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People with OUD are not suffering from a medical DISEASE treatable with medication from a self-imposed condition maintained through the lack of moral fiber.\n",
      "The total input tokens: 1958069\n",
      "The average input tokens: 6316.351612903226\n"
     ]
    }
   ],
   "source": [
    "# crafting the prompt\n",
    "myth = MYTH_TO_STATEMENT[myth_key]\n",
    "print(myth)\n",
    "few_shot_examples = prompts.M2_FEW_SHOT_EXAMPLES\n",
    "crafted_prompt = EvaluatorHelper.create_myth_specific_prompts(df, prompt_chosen, prompts.persona, myth, few_shot_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc8e26a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "Total Number of Input Token: 1931562\n",
      "Total Number of Output Token: 61949\n",
      "Elapsed time: 1042.112122 seconds\n"
     ]
    }
   ],
   "source": [
    "# feeding the prompt into the LLM and collecting the output\n",
    "start = time.perf_counter()\n",
    "vid_to_output, total_completion_token, total_prompt_token = GPTRequests.evaluate_prompts(client, crafted_prompt, model_name, temperature)\n",
    "end = time.perf_counter()\n",
    "print(f\"Elapsed time: {end - start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b472680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vid_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fde3f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTRequests.extract_and_save_output(save_file_dir_EVAL, vid_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_dir_EVAL = EVAL_DIR + model_name + '-' + myth_key + '-few-shot-evaluation.json'\n",
    "with open(save_file_dir_EVAL, 'r') as json_file:\n",
    "    vid_to_output = json.load(json_file)\n",
    "print(save_file_dir_EVAL)\n",
    "    \n",
    "M10_few_shot_exclude = ['6F6d10ggVDw', 'nmMCQ1y8l14', 'GI3blNNe56w', 'E9jKyHjPbUg', 'QtRQ9UD7rpY']\n",
    "EvaluatorHelper.compute_results(vid_to_output, vid_to_label_eval, False, M10_few_shot_exclude)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
